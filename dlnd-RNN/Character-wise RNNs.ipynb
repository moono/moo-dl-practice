{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-wise RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare packages\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open text\n",
    "anna_text_fn = '../data/anna.txt'\n",
    "def open_text_file(fn):\n",
    "    with open(anna_text_fn, 'r') as f:\n",
    "        t = f.read()\n",
    "    return t\n",
    "text = open_text_file(anna_text_fn)\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  83\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# get unique characters\n",
    "vocab = set(text)\n",
    "print('vocabulary size: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mark ids to characters\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = {i: c for c, i in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 51, 68, 46, 77, 41, 13, 45, 73, 50, 50, 50, 69, 68, 46, 46, 61,\n",
       "       45,  6, 68, 71, 40,  8, 40, 41, 42, 45, 68, 13, 41, 45, 68,  8,  8,\n",
       "       45, 68,  8, 40, 25, 41, 62, 45, 41, 11, 41, 13, 61, 45, 23, 15])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode anna text in np.array\n",
    "encoded = np.array( [vocab_to_int[c] for c in text], dtype=np.int32 )\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: the number of sequences per batch(sequence length)\n",
    "       n_steps: Number of sequence steps per batch(how many sequence needed for single batch?)\n",
    "    '''\n",
    "    #print('Input arr length: ', len(arr))\n",
    "    \n",
    "    # get number of chracters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # keep only enough chracters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    #print('cliped arr length: ', len(arr))\n",
    "    #print('characters_per_batch: ', characters_per_batch)\n",
    "    #print('batch size: ', n_batches)\n",
    "    \n",
    "    # reshape\n",
    "    arr = arr.reshape( (n_seqs, -1) )\n",
    "    #print('reshaped arr: ', arr.shape)\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # the features\n",
    "        x = arr[:, n:n + n_steps]\n",
    "        # targets, x shifted by 1\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[33 51 68 46 77 41 13 45 73 50]\n",
      " [45 68 71 45 15 24 77 45 79 24]\n",
      " [11 40 15 70 50 50 39 37 41 42]\n",
      " [15 45 30 23 13 40 15 79 45 51]\n",
      " [45 40 77 45 40 42 36 45 42 40]\n",
      " [45  2 77 45 16 68 42 50 24 15]\n",
      " [51 41 15 45 28 24 71 41 45  6]\n",
      " [62 45 55 23 77 45 15 24 16 45]\n",
      " [77 45 40 42 15 63 77 70 45 29]\n",
      " [45 42 68 40 30 45 77 24 45 51]]\n",
      "\n",
      "y\n",
      " [[51 68 46 77 41 13 45 73 50 50]\n",
      " [68 71 45 15 24 77 45 79 24 40]\n",
      " [40 15 70 50 50 39 37 41 42 36]\n",
      " [45 30 23 13 40 15 79 45 51 40]\n",
      " [40 77 45 40 42 36 45 42 40 13]\n",
      " [ 2 77 45 16 68 42 50 24 15  8]\n",
      " [41 15 45 28 24 71 41 45  6 24]\n",
      " [45 55 23 77 45 15 24 16 45 42]\n",
      " [45 40 42 15 63 77 70 45 29 51]\n",
      " [42 68 40 30 45 77 24 45 51 41]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "    '''\n",
    "    inputs = tf.placeholder( tf.int32, [batch_size, num_steps], name='inputs' )\n",
    "    targets = tf.placeholder( tf.int32, [batch_size, num_steps], name='targets' )\n",
    "    \n",
    "    # keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder( tf.float32, name='keep_prob' )\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "    '''\n",
    "    # build lstm cell with dropout\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # stack up multiple LSTM layers\n",
    "    cell = tf.contrib.rnn.MultiRNNCell( [build_cell(lstm_size, keep_prob) for _ in range(num_layers)] )\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    '''\n",
    "    # reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable( tf.truncated_normal([in_size, out_size], stddev=0.1) )\n",
    "        softmax_b = tf.Variable(  tf.zeros(out_size))\n",
    "    \n",
    "    # logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits=logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets        \n",
    "    '''\n",
    "    # one-hot encoded targets and reshape to match logitsm one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # softmax cross entrophy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learnin_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer    \n",
    "    '''\n",
    "    # optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learnin_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                 lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                 grad_clip=5, sampling=False):\n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Here I'm defining the hyperparameters for the network.\n",
    "* batch_size - Number of sequences running through the network in one pass.\n",
    "* num_steps - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn * more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* lstm_size - The number of units in the hidden layers.\n",
    "* num_layers - Number of hidden LSTM layers to use\n",
    "* learning_rate - Learning rate for training\n",
    "* keep_prob - The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4209...  0.3951 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3325...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8526...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.2568...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.3578...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.9851...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.8226...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.6760...  0.1640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.5467...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4759...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.4065...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3933...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3818...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3906...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3634...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3437...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.3201...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3399...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.3110...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2646...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2898...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2679...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2700...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2668...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2372...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2541...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2519...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2246...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2227...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2438...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2512...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2187...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1996...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2305...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1905...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2150...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1805...  0.1531 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1847...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1839...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1820...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1777...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1803...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1776...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1735...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1626...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1815...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1825...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1881...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1803...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1765...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1690...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1647...  0.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1673...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1549...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1610...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1448...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1517...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1533...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1460...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1567...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1557...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1722...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1819...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1272...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1348...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1657...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1509...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1133...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1291...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1527...  0.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1487...  0.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1660...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1401...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1437...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1483...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1527...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1432...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1318...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1328...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1241...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1188...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1458...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1392...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1275...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1094...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1192...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1127...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1059...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1306...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1272...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1231...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.1104...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1147...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1183...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1008...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1033...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1108...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.0983...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1001...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0955...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.0996...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0992...  0.1510 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0997...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0879...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0928...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0929...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0609...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0717...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0749...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0536...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0615...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0634...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.0507...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0491...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0531...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0472...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0451...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0641...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0623...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0309...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0581...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0303...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0194...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0251...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.0104...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 2.9878...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.0121...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.0129...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 2.9952...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.0012...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 2.9959...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9652...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9641...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9569...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9124...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9150...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9529...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.9172...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9550...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.9158...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.9163...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8859...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.8800...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8716...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8654...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8712...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8613...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8785...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.8281...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.8266...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8566...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8611...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8096...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8217...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.8086...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.7818...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7685...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7630...  0.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.7313...  0.1506 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7564...  0.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7375...  0.1530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.6996...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.6857...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.7147...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.7077...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.6996...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.6952...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.7201...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.6954...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.6617...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.6723...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.6870...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.6900...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.6767...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.6778...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.7383...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.6770...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.6162...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.6259...  0.1540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.6053...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.6135...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.6211...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.5994...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.6085...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.6463...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.5967...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.5637...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5573...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5588...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5534...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5677...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5278...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5497...  0.1510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5277...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5199...  0.1520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.5297...  0.1500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.5176...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.5056...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5725...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4916...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.5047...  0.1630 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.5167...  0.1520 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.5040...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.5013...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.5007...  0.1690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.5042...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.5241...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4802...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4751...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4849...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4822...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.5042...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4770...  0.1501 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4754...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4746...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.5086...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4723...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4483...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4581...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4922...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4628...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4476...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4366...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4497...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4395...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4409...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4570...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4513...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4625...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4224...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.4132...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4486...  0.1503 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.4136...  0.1494 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4308...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.4116...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3945...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.4022...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.4061...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.4110...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.4092...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3906...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3935...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3956...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3572...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.4127...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3927...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3831...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.4138...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3632...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3999...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3902...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3695...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3736...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3837...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3700...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3690...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3601...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3929...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3684...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3726...  0.1690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3857...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3607...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3474...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3842...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3547...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3227...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3291...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3555...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3575...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3637...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3457...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3311...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3345...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3797...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3375...  0.1491 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3467...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.3100...  0.1533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.3151...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.3068...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3441...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.3046...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2949...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2733...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.3012...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.3116...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.3030...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2919...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.3096...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2906...  0.1640 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.3058...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2931...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2902...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2679...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2870...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2773...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2819...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2723...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2670...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.3098...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2880...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2660...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2693...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2680...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2755...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2770...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2965...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2876...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2520...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2772...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2881...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2550...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2474...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2503...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.2175...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2622...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2499...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2799...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2503...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2777...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2449...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2337...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2700...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2505...  0.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.2142...  0.1660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2466...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.2466...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2463...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2371...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.2280...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.2099...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2404...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2489...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.2342...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2380...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2264...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.2342...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2633...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.2173...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2475...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.2171...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.2153...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.2146...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.2154...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2337...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.2308...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2340...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.2132...  0.1670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1940...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.2176...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2475...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.2088...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.2140...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1902...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.2017...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1887...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1858...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1616...  0.1566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2295...  0.1548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.2034...  0.1530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1838...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1881...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1921...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1939...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1855...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1982...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.2009...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1753...  0.1540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1710...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1720...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1863...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.2137...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.2027...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1842...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1986...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1908...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1863...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1594...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1486...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1607...  0.1490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1750...  0.1660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1754...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1844...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.2058...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1640...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1564...  0.1520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1404...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1469...  0.1550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1478...  0.1539 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1681...  0.1577 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.1192...  0.1573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1567...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1472...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.1205...  0.1500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1477...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1462...  0.1510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.1207...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.2155...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.1232...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.1115...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.1236...  0.1620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.1241...  0.1730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.1087...  0.1550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.1251...  0.1544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.1314...  0.1555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1561...  0.1572 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.1309...  0.1542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.1149...  0.1555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.1030...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.1248...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1595...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.1119...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0987...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.1140...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1510...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.1122...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.1095...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.1059...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1555...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.1015...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0941...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.1039...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0827...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0813...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.1053...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1356...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.1094...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.1053...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0738...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0873...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.1259...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0822...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0896...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0911...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0397...  0.1630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.0492...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0490...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0676...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0718...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0593...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0560...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0757...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 2.0141...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0880...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0435...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0583...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.1054...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0352...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.1058...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0699...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0616...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0444...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0695...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0654...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0581...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0498...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0928...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0559...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0811...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0919...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0589...  0.1501 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0402...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0757...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0586...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 2.0203...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0333...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0496...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0762...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0504...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0636...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.0275...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0317...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0864...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0451...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0495...  0.1509 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 2.0069...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.0232...  0.1545 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 2.0000...  0.1527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0478...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 2.0075...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 2.0182...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9945...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 2.0037...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 2.0206...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 2.0029...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9852...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0296...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9969...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 2.0132...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9823...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9862...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9870...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 2.0041...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 2.0051...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9848...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9883...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9563...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 2.0190...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 2.0102...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9852...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9889...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9937...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9994...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9959...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 2.0101...  0.1519 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 2.0088...  0.1545 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 2.0068...  0.1565 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9975...  0.1670 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9894...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9883...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9804...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9757...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9441...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9909...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9712...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9935...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9901...  0.1546 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 2.0038...  0.1521 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9545...  0.1495 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9615...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 2.0051...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9857...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9387...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9920...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9894...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9837...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9865...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9553...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9483...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9880...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9873...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9707...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9758...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9795...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9680...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 2.0070...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9602...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9958...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9671...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9725...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9555...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9489...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9838...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9853...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9948...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9771...  0.1578 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9477...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9591...  0.1529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9993...  0.1507 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9729...  0.1516 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9622...  0.1516 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9573...  0.1505 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9523...  0.1504 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9555...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9537...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.9192...  0.1551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9873...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9799...  0.1567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9492...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9717...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9596...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9463...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9396...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9662...  0.1520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9938...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9303...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9472...  0.1540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.9387...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9395...  0.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9741...  0.1610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9601...  0.1550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9610...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9509...  0.1623 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.9312...  0.1564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9512...  0.1542 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.9168...  0.1535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.9071...  0.1528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.9147...  0.1545 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9424...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.9333...  0.1535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9603...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9467...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.9140...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9255...  0.1576 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.9012...  0.1551 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.9200...  0.1555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.9358...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9380...  0.1589 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8955...  0.1505 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9280...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.9102...  0.1529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8882...  0.1585 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9295...  0.1516 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.9194...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.9041...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9971...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.9029...  0.1550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8974...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.9082...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.9000...  0.1625 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8630...  0.1521 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.9051...  0.1942 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8941...  0.1630 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9436...  0.1555 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.9001...  0.1534 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8955...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8879...  0.1525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.9029...  0.1515 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9493...  0.1629 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8956...  0.1579 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8735...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.9024...  0.1620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9443...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.9068...  0.1630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.9032...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8885...  0.1840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9452...  0.1670 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8965...  0.1620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8862...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8919...  0.1620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8688...  0.1610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8798...  0.1644 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.9000...  0.1595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9171...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.9006...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8941...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8578...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8906...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.9097...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8742...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8786...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8675...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8409...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8380...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8543...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8576...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8839...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8539...  0.1840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8474...  0.1660 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8866...  0.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8302...  0.1720 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8721...  0.1720 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8540...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8556...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8985...  0.1690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8352...  0.1550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.9256...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8600...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8613...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8497...  0.1516 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8778...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8908...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8551...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8444...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.9092...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8744...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.9079...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8873...  0.1522 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8746...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8603...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8920...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8730...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8448...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8558...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8509...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8982...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8777...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8842...  0.1789 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8378...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8519...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8798...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8661...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8671...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.8139...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8430...  0.1537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.8137...  0.1497 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8692...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.8070...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8437...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.8103...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8309...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8347...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.8176...  0.1610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.8130...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8507...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.8149...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.8289...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.8195...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.8215...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.8118...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8347...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.8315...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.8182...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.8069...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7900...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8398...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.8238...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.8132...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.8220...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.8204...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.8205...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.8348...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8409...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8364...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8325...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.8178...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.8078...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.8126...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.8045...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7930...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7809...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8204...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.8082...  0.1511 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.8188...  0.1610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.8086...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8316...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7725...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7797...  0.1640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8312...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.8065...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7704...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8343...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.8332...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.8103...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.8076...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7871...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7771...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.8266...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.8194...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.8186...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.8109...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.8191...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.8065...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8335...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.8028...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8414...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7974...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.8104...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.8100...  0.1550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7853...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.8322...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.8141...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8382...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.8082...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.8021...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7869...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.8217...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.8137...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.8078...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7928...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7887...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.8123...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.8009...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7738...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.8217...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8284...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7928...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.8214...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.8038...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7922...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7858...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.8051...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8547...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7823...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7883...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7714...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7748...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.8144...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7955...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7979...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7808...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7718...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.8029...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7658...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7450...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7560...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7791...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7722...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.8019...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7786...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7660...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7857...  0.1530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7637...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7747...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7808...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7799...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7419...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7828...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7511...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7342...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7758...  0.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7580...  0.1520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7642...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8670...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7558...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7603...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7667...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7611...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.7247...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7705...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7532...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7930...  0.1720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7530...  0.1500 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7433...  0.1540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7409...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7610...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7988...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7528...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7333...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7570...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7927...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7625...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7587...  0.1600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7427...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7836...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7524...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7647...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7736...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.7223...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.7325...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7671...  0.1640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7740...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7615...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7653...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.7178...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7652...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7772...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7317...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7491...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7232...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.7010...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.7052...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.7192...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.7228...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7668...  0.1540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.7222...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.7079...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7478...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.7009...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7322...  0.1550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.7216...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.7305...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7752...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.7153...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7938...  0.1550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7347...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7367...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7266...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7463...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7574...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.7151...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.7106...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7626...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7396...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7711...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7591...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7513...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7308...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7589...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7514...  0.1540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.7246...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7266...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.7188...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7676...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7454...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7520...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.7195...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.7180...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7515...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7283...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.7340...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6844...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.7202...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6864...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7373...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6749...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.7124...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6829...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.7039...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6904...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6928...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6744...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7296...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6986...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.7104...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6931...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6924...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.7010...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.7136...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.7102...  0.1650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6774...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6869...  0.1550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6666...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.7149...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6926...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6834...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.7046...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6906...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.7023...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.7055...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.7123...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.7058...  0.1550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.7235...  0.1740 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6983...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6989...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6984...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6915...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6766...  0.1540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6563...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.7027...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6896...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6930...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6912...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.7035...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6743...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6587...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.7119...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6946...  0.1540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6551...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.7161...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.7068...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6838...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6791...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6573...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6548...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.7086...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6982...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6931...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6969...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7099...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.7000...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.7133...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6760...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7447...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6861...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6971...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.7095...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6747...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.7109...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6962...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.7234...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6941...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6818...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6621...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6992...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6951...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6950...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6840...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6742...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6982...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6896...  0.1541 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6404...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.7141...  0.1550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.7122...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6908...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6994...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6833...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6730...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6778...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.7013...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7487...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6791...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6709...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6592...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6537...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.7026...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6994...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6949...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6698...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6619...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6865...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6560...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6387...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6499...  0.1830 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6741...  0.1600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6808...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6988...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6908...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6627...  0.1520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6803...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6558...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6718...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6809...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6807...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6490...  0.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6752...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6411...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6313...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6734...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6580...  0.1760 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6517...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7614...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6679...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6531...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6662...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6421...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6193...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6573...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6406...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6797...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6512...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6289...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6440...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6645...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6985...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6454...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6328...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6602...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6824...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6560...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6583...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6561...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6874...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6428...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6571...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6595...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.6195...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.6298...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6684...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6761...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6622...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6527...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6293...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6669...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6660...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6396...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6497...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6209...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.5998...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.6075...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.6263...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6175...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6793...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6258...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.6162...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6529...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5991...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6303...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6230...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.6281...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6732...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.6137...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6865...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6335...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6366...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6315...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6399...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6594...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6271...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.6221...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6678...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6362...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6902...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6654...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6463...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6329...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6546...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6539...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.6119...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6388...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6310...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6683...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6499...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6655...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6183...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6230...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6565...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6234...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6304...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5918...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.6282...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5899...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6376...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5821...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6263...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.6067...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.6061...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5944...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5924...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5874...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6428...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5923...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.6021...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5927...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5930...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5924...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6262...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.6100...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5770...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.5937...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5771...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.6207...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.6103...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.6014...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.6207...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.6009...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.6172...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.6154...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.6140...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.6202...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6351...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.5990...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.6013...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.6131...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.6012...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5834...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5746...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.6190...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.6065...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.6073...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.5998...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.6162...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5658...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5677...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.6164...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.6036...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5686...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.6239...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.6219...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5919...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5795...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5668...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5780...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6175...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.6157...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.6107...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.6140...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.6321...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.6103...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.6143...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.5966...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6525...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.5994...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.5983...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6250...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.5932...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.6209...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.6104...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6364...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.6244...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5909...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5630...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.5979...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.6140...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.6079...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.5932...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5932...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.6111...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.5962...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5609...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.6200...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6350...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.5975...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.6151...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.5998...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5926...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.5878...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.6137...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6697...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5982...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5856...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5782...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5678...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.6155...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.5925...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.6160...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5689...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5818...  0.1540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.6114...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5705...  0.1620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5584...  0.1520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5618...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5880...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5875...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.6036...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5902...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5767...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.6073...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5761...  0.1530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5809...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5971...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5754...  0.1690 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5596...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5891...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5551...  0.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5515...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5985...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5737...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5754...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6825...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5887...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5836...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.5921...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5701...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5360...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5822...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5612...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5965...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5673...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5520...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5589...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5727...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.6123...  0.1540 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5707...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5629...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5837...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.6046...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5820...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5936...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5696...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.5934...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5554...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5731...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5847...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5326...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5361...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5862...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5975...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5897...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5695...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5353...  0.1620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5880...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5855...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5730...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5655...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5480...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5235...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.5261...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5448...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5638...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.6014...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5491...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5370...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5849...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5276...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5587...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5570...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5559...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5885...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5429...  0.1630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.6167...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5685...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5681...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5603...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5733...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5858...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5464...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5513...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5991...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5710...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.6089...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5946...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5788...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5613...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5790...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5771...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5418...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5638...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5536...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.6095...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5724...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5928...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5473...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5492...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5783...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5558...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5550...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.5158...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5608...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.5202...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5617...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.5131...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5451...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.5346...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5399...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.5288...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5349...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.5115...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5612...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.5367...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5323...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.5207...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5304...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.5258...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5566...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5481...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.5054...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.5292...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.5102...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5521...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5386...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5394...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5372...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5376...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5495...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5495...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5446...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5402...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5560...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.5247...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5431...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5506...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5300...  0.1520 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.5138...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.5069...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5474...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5426...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5337...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.5302...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5373...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.5038...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.5018...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5528...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5427...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.5043...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5531...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5500...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5296...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.5126...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.5016...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.5108...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5599...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5471...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5485...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5478...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5604...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5549...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5522...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5404...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5888...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5397...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.5387...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5594...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.5268...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5589...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5469...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5728...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5607...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.5278...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4976...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.5425...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5495...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5358...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5368...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5417...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5413...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.5255...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4942...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5524...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5658...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5320...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5394...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5308...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5346...  0.1530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5331...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5546...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.6075...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.5383...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.5249...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5273...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.5182...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5530...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5332...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5574...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.5088...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.5235...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5473...  0.1550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.5093...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4952...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.5045...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.5241...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.5367...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.5307...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.5166...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.5094...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5468...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.5169...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.5248...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.5344...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.5100...  0.1630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.5068...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.5248...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.5071...  0.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4918...  0.1520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.5254...  0.1500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.5087...  0.1540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.5122...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.6211...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5200...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.5226...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5307...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.5084...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4964...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.5297...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.4918...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5361...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.5162...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4892...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.5092...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.5173...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5416...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.5117...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4969...  0.1540 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.5286...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5390...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.5306...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5391...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.5074...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5336...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.5120...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.5219...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.5274...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4812...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4777...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.5279...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.5297...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.5411...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.5044...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4827...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.5273...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.5298...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.5066...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.5141...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.5024...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4779...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4698...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.5040...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.5021...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5494...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4857...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4796...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.5273...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4771...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4956...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4919...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.5093...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.5222...  0.1521 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4827...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5483...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.5102...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.5169...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.5127...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.5102...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.5258...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4788...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4866...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5392...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.5084...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5547...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.5361...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.5130...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.5098...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.5098...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.5115...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4955...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.5052...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4977...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5492...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.5313...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5349...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4940...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.5064...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.5222...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4950...  0.1550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4935...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4629...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.4976...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4482...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.5058...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4586...  0.1690 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4992...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4724...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4969...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4717...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4817...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4722...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.5110...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4832...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4851...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4764...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4750...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4652...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4981...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4966...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4556...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4764...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4580...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4965...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4822...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4836...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4784...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4902...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4923...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4962...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4961...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4995...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.5027...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4780...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4782...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4856...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4781...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4657...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4556...  0.1650 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4858...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4939...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4812...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4784...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4858...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4530...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4463...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4968...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4862...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4443...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4935...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4908...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4675...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4514...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4425...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4559...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.4954...  0.1650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4999...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4860...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4941...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.4992...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.5015...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.5023...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4888...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5455...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4926...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4828...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.5141...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4755...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.4987...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.5064...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.5252...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.5140...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4745...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4468...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4788...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4968...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4772...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4857...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4850...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4895...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4768...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4415...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.5056...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.5112...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4767...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4881...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4884...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4858...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4870...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4982...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5545...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4834...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4806...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4664...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4531...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.5162...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4773...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.5015...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4541...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4582...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4919...  0.1511 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4605...  0.1540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4363...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4535...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4682...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4721...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4761...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4720...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4614...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.5023...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4728...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4742...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4742...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4668...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4574...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4654...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4569...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4323...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4935...  0.1500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4684...  0.1530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4607...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.5745...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4807...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4705...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4789...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4426...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4300...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4775...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4546...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4827...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4567...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4531...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4547...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4665...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4777...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4546...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4477...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4746...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4883...  0.1550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4806...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4903...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4552...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4925...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4610...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4876...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4694...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.4274...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.4442...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4847...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4804...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4829...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4621...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4316...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4753...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4774...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4526...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4647...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4411...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.4219...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.3983...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4503...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4490...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4941...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4442...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4357...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4683...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.4270...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4449...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4401...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4473...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4825...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4222...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4933...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4559...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4666...  0.1720 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4493...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4566...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4760...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4469...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4338...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4844...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4632...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.4961...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4775...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4623...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4546...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4683...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4685...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4356...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4481...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4473...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4956...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4813...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4807...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4395...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4467...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4676...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4407...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4454...  0.1561 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.4090...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4488...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.4071...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4406...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.4039...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4383...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.4268...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4310...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.4130...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4177...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.4141...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4494...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4335...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4318...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.4135...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.4275...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.4321...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4479...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4445...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.4137...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.4253...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.4005...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4540...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4335...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4407...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4323...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4355...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4424...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4502...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4543...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4411...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4620...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.4233...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4377...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4324...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4169...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.4160...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3994...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4429...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4413...  0.1530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4250...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4365...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4364...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.4154...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3990...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4385...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4432...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.4052...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4566...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4530...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.4338...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.4056...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3972...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.4130...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4674...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4620...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4386...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4391...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4610...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4565...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4549...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4399...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4907...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4407...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4302...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4645...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.4235...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4602...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4371...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4705...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4610...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4352...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3960...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.4241...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4577...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4446...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4368...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4275...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4444...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.4409...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3997...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4460...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4763...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4326...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4312...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4425...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4357...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4329...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4565...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.5080...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4358...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4309...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4235...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.4070...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4543...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4209...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4316...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.4128...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.4060...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4655...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.4075...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3921...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.4052...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.4207...  0.1540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4325...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.4276...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.4227...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.4169...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4474...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.4131...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.4193...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.4284...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.4081...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3994...  0.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.4189...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.3961...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3883...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4313...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.4230...  0.1510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.4064...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5466...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4410...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4364...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4299...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.3991...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3938...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4312...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.4146...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4323...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4228...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.4101...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.4234...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4209...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4479...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.4156...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.4019...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4373...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4356...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4362...  0.1690 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4354...  0.1520 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.4215...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4426...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.4143...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4322...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4211...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3802...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3914...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4371...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4262...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4366...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.4072...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3878...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4346...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4356...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.4060...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.4102...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3979...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3711...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3645...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3887...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3888...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4517...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3833...  0.1660 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3857...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4264...  0.1491 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3821...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.4152...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.4099...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.4205...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4307...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3902...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4501...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.4194...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.4115...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.4161...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.4237...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4346...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3923...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3894...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4438...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.4221...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4494...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4358...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4329...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.4115...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4196...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.4243...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3884...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.4076...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.4001...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4565...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4351...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4455...  0.1700 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3853...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.4070...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4293...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.3984...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.4014...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3691...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.4239...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3578...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.4026...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3683...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.4033...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3866...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3953...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3827...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3800...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3691...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.4184...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3823...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3909...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3874...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3727...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3709...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.4172...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.4048...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3712...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3713...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3642...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3999...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.4016...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.4004...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3870...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3880...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.4021...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.4049...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.4026...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3980...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.4109...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3804...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.4109...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.4018...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3879...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3712...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3586...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.4007...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.4010...  0.1530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3955...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3839...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3956...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3617...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3369...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.4008...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3909...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3490...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.4042...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.3980...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3818...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3618...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3485...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3707...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.4133...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.4038...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.4096...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3970...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4233...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.4098...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.4062...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.3913...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4540...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.3994...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3876...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4354...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3709...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.4114...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.3971...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4339...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4162...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3983...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3624...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3795...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.4088...  0.1530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3921...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3965...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3827...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3945...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3906...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3543...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4148...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4281...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3960...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.4035...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.4076...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.3928...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3935...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4141...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4565...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.3938...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3969...  0.1520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3856...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3684...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4194...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3884...  0.1680 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.4084...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3781...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3781...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.4147...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3689...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3583...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3595...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3930...  0.1670 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3880...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3794...  0.1550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3912...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3721...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.4252...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3777...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3896...  0.1540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3903...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3705...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3680...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3877...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3657...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3516...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3932...  0.1510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3812...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3748...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5333...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.4036...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3837...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.4010...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3654...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3508...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.3872...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3672...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.3854...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3911...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3677...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3859...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3876...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.4022...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3738...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3637...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.3930...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.3995...  0.1510 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3868...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.4113...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3730...  0.1640 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.4035...  0.1710 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3703...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.3918...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3902...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3344...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3591...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.4051...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.3961...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.4043...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3778...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3511...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3827...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3911...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3641...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3841...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3639...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3412...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3244...  0.1545 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3642...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3566...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.4192...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3682...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3537...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3875...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3501...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3701...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3598...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3652...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3934...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3506...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.4186...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3727...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.3877...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3655...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3789...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.4064...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3623...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3544...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.4117...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3799...  0.1720 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4250...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.3959...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3785...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3697...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3899...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3876...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3624...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3669...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3643...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4220...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.4014...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.4071...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3583...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3836...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3859...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3649...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3651...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3433...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3830...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3367...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3746...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3402...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3693...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3500...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3683...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3489...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3420...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3324...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3748...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3538...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3561...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3509...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3469...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3498...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3868...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3712...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3327...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3415...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3288...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3726...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3578...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3703...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3620...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3585...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3602...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3701...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3842...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3591...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3882...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3583...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3732...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3701...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3499...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3473...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3306...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3786...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3776...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3574...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3667...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3632...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3343...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3139...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3709...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3592...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3199...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3826...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3811...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3585...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3317...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3136...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3351...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3751...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3715...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3622...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3598...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.3878...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3790...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3726...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3689...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4196...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3748...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3627...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.3925...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3516...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3808...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3739...  0.1690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.4067...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3863...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3648...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3315...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3484...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3817...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3481...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3656...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3590...  0.1521 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3693...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3540...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3327...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3816...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.3987...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3606...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3619...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3694...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3599...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3688...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3872...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4277...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3691...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3600...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3516...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3480...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3888...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3656...  0.1502 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3734...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3410...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3500...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3916...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3348...  0.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3305...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3399...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3513...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3578...  0.1530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3521...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3497...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3468...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3873...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3553...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3516...  0.1550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3515...  0.1520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3350...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3368...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3596...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3391...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3240...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3621...  0.1510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3544...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3409...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.5278...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3760...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3555...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3720...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3396...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3254...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3677...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3524...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3653...  0.1720 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3530...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3478...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3539...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3607...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3793...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3438...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3339...  0.1480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3666...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3896...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3645...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3785...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3576...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3740...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3455...  0.1630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3575...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3578...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.3124...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3327...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3732...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3693...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3672...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3479...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3222...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3702...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3659...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3380...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3564...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3246...  0.1680 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.3116...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.2960...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3339...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3330...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3887...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3419...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3170...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3641...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3184...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3450...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3425...  0.1504 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3502...  0.1496 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3605...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3218...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.4010...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3653...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3573...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3436...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3454...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3645...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3378...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3232...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.3778...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3651...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.3982...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3683...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3624...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3540...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3620...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3610...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3349...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3489...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3356...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.3852...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3734...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3768...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3362...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3445...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3706...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3465...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3355...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.3080...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3532...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.3089...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3430...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.3159...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3428...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3307...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3449...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3153...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3132...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3065...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3512...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3224...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3318...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3254...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3269...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3241...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3516...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3412...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3107...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3196...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3135...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3524...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3432...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3415...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3321...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3314...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3333...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3449...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3425...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3394...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3619...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3197...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3471...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3459...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3290...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3143...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.3116...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3487...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3538...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3291...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3309...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3418...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.3132...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.2911...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3496...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3357...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.2951...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3517...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3540...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3297...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.2996...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.3015...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3218...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3603...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3486...  0.1670 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3450...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3474...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3652...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3586...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3429...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3478...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.3805...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3518...  0.1620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3368...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3805...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3248...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3639...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3407...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3721...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3561...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3411...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.3016...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3181...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3453...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3377...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3319...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3395...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3435...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3322...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.3045...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3655...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3754...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3377...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3369...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3383...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3390...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3313...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3653...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.4031...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3360...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3369...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3282...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3210...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3791...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3376...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3434...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.3197...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3296...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3613...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3164...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.3072...  0.1530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3123...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3370...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3312...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3304...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3342...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3140...  0.1520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3593...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3377...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3320...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3337...  0.1550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.3049...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3131...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3295...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.3165...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.2989...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3399...  0.1510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3217...  0.1540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3236...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4903...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3608...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3312...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3586...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3256...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.2969...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3314...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3240...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3445...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3181...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3281...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3342...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3342...  0.1650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3550...  0.1500 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3202...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3188...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3494...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3591...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3350...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3507...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3382...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3597...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3268...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3404...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3381...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.2797...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.3015...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3460...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3429...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3457...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3263...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.3007...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3465...  0.1620 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3413...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3190...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3439...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.3082...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2828...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2821...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3232...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.3027...  0.1640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3674...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3109...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.3049...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3327...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.3080...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3260...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3185...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3259...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3457...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.2972...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3705...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3342...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3392...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3268...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3352...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3493...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3180...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.2968...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3594...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3361...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3786...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3544...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3340...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3212...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3389...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3523...  0.1502 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3129...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3405...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.3181...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3755...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3524...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3600...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3072...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3305...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3560...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3276...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3214...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.2927...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3285...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2850...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3255...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.3038...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3145...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.3028...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3215...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.3109...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.3059...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.2937...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3326...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.3050...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3116...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.3005...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.3036...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.3012...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3306...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3315...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.2971...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.3048...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.2974...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3183...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3214...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3277...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3205...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3227...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3217...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3160...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3259...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3060...  0.1630 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3374...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.3072...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3185...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3286...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3066...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.2946...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2801...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3235...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3300...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3044...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3080...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3150...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2833...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2685...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3215...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.3198...  0.1710 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2784...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3302...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3196...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.2975...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2778...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2706...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.3121...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3383...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3258...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3178...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3185...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3385...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3386...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3298...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3241...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3720...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3282...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3177...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3472...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.3138...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3441...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3257...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3644...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3525...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.3198...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2833...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.2963...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3322...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3156...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3195...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3124...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3284...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3141...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2870...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3315...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3518...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3201...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3134...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3304...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3138...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3189...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3407...  0.1690 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3797...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3286...  0.1550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3226...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.3064...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.3021...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3459...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3183...  0.1561 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3260...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.2978...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.2981...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3519...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.2937...  0.1540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2853...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.3013...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3141...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3196...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.3090...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.3131...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.2971...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3377...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.3073...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.3058...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.3194...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2834...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.2940...  0.1500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3244...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.3022...  0.1520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2826...  0.1530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3189...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.3048...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.3040...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4796...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3437...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3162...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3270...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.2973...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2781...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3216...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.3102...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3119...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3154...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.3006...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3144...  0.1550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3147...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3311...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.3037...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2858...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3414...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3406...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3271...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3415...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3115...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3289...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.3089...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3185...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3185...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2729...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2950...  0.1730 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3300...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3234...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3283...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.2998...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2802...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3126...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3189...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2972...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3080...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.2865...  0.1660 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2644...  0.1650 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2601...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.2984...  0.1620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2844...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3531...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.2849...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2817...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3290...  0.1630 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2727...  0.1640 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.2893...  0.1740 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.2964...  0.1720 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.3057...  0.1650 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3278...  0.1700 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2834...  0.1680 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3526...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3142...  0.1770 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3150...  0.1690 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.3028...  0.1670 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3071...  0.1660 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3269...  0.1630 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.3003...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2849...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3509...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3135...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3548...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3364...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3133...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.3032...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3263...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3184...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.2944...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3155...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2951...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3481...  0.1620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3377...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3453...  0.1620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2875...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.3086...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3203...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3016...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.2997...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2621...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3082...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2672...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3047...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2745...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.3022...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2849...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.3056...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2838...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2865...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2661...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3188...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2882...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.2933...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2886...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2800...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2925...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3152...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.3032...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2709...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2867...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2762...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.3079...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.2896...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.3073...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.2885...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.3081...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.3036...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.3013...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3046...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2910...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.3130...  0.1550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2900...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.3019...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.3026...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.2880...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2759...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2653...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.3044...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3211...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.3020...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.2934...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.2946...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2604...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2401...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3051...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2910...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2522...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3153...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.3120...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2837...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2553...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2523...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2884...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3199...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.3095...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.3064...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.2994...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3235...  0.1690 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3223...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3136...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.3117...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3530...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3127...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2989...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3351...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2899...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3216...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.3086...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3258...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3294...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2963...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2721...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2776...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3124...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.2890...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.2965...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.2900...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.3034...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2958...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2708...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3309...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3395...  0.1540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.3152...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2927...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2996...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.3097...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.2957...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3226...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3647...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3122...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.3015...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.2888...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2827...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3331...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.2991...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.3078...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2726...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2833...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3283...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2780...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2606...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2832...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2896...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.2955...  0.1584 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2841...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.2925...  0.1529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2776...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3257...  0.1550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2960...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2863...  0.1554 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2886...  0.1542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2766...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2786...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2911...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2774...  0.1500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2592...  0.1530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.3031...  0.1520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2872...  0.1509 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2849...  0.1544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4473...  0.1536 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3261...  0.1506 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.3004...  0.1586 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3206...  0.1528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2841...  0.1515 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2669...  0.1561 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.3002...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2887...  0.1524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.3022...  0.1606 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2853...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2751...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2964...  0.1506 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.3090...  0.1517 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.3067...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2890...  0.1650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2706...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3121...  0.1528 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3243...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.3057...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3170...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.2909...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3169...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2847...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3072...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.2963...  0.1522 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2498...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2627...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3094...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.3047...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3109...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2851...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2585...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.2962...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.3055...  0.1559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2812...  0.1513 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.2953...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2642...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2409...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2547...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2826...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2730...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3479...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2721...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2673...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.2973...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2658...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2872...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2810...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2940...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.3101...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2597...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3264...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.2959...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.2988...  0.1640 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2852...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.2935...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3112...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2789...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2708...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3368...  0.1631 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.2924...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3401...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3030...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.3016...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.2883...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.3014...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3023...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2805...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2948...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2735...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3406...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3054...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3164...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2663...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.2867...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.3005...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2896...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2784...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2526...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.2955...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2610...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.2938...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2493...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2833...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2708...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2869...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2656...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2630...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2608...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.2988...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2668...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2833...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2567...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2621...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2663...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.3025...  0.1610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2987...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2558...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2728...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2667...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2859...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2743...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2932...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2851...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2812...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2859...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2847...  0.1711 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2987...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2749...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.3012...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2729...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.2805...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2900...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2810...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2557...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2485...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2944...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.3027...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2850...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2820...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2790...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2445...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2408...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2830...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2742...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2476...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2910...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2870...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2711...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2454...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2335...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2726...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.3030...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.2880...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.2932...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2844...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3142...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.2994...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.2968...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.2914...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3364...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.2869...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2853...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3144...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2738...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3162...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2929...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3200...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3037...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2881...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2581...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2719...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.3015...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2722...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2834...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2813...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2939...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2709...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2528...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.3040...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.3109...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2808...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2772...  0.1540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2846...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2788...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2811...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3069...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3451...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2999...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2962...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2762...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2716...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3198...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2832...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.3056...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2643...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2632...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3218...  0.1670 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2662...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2634...  0.1550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2738...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2875...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2820...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2757...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2776...  0.1530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2655...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3122...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2800...  0.1510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2741...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2754...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2554...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2645...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2820...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2609...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2523...  0.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2823...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2727...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2594...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.4434...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.3048...  0.1640 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2798...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.3079...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2601...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2404...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2732...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2784...  0.1510 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2867...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2645...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2681...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2703...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2898...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2922...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2610...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2641...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2915...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.3001...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2801...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.3081...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2806...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.3050...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2643...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.2953...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2822...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2430...  0.1550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2524...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.2922...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.2867...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.3015...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2723...  0.1740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2500...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2872...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2862...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2646...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2889...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2590...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2295...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2311...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2681...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2537...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3210...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2575...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2406...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2829...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2560...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2600...  0.1630 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2785...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2880...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.2886...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2390...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3149...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2775...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2975...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2679...  0.1554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2788...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2902...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2636...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2536...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.3064...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2882...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3137...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.2951...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2823...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2687...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2939...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.2942...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2598...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2782...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2662...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3200...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.2992...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3048...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2611...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2732...  0.1630 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.2986...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2697...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2714...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2342...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2820...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2474...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2773...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2415...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2771...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2581...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2782...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2482...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2561...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2373...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2748...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2561...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2661...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2579...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2528...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2542...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2872...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2751...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2440...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2530...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2563...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2730...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2568...  0.1610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2775...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2704...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2632...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2603...  0.1530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2727...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2689...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2585...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2840...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2578...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2797...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2771...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2632...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2389...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2433...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2663...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2861...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2668...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2702...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2656...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2436...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2270...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2620...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2575...  0.1550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2307...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2867...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2737...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2476...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2292...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2217...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2488...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2897...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2770...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2830...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2811...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.2900...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2867...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2849...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2756...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3267...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2878...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2519...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.3061...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2536...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.2910...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2779...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.3069...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.2988...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2700...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2415...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2497...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2879...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2578...  0.1610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2577...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2681...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2806...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2596...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2390...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2975...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2948...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2755...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2738...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2696...  0.1550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2715...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2634...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.3037...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3286...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2844...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2722...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2665...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2498...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.2992...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2723...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2762...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2545...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2681...  0.1670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.3040...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2534...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2435...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2515...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2661...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2699...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2597...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2532...  0.1540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2408...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2962...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2628...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2687...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2713...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2432...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2460...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2703...  0.1530 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2626...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2306...  0.1520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2629...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2573...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2587...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.4173...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2928...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2725...  0.1620 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.2827...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2563...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2236...  0.1500 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2638...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2639...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2662...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2620...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2562...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2690...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2735...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2790...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2371...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2365...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2754...  0.1540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2681...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2664...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2913...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2548...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2855...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2508...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2799...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2777...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2230...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2308...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2871...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2722...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2767...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2551...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2373...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2742...  0.1756 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2660...  0.1542 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2573...  0.1551 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2672...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2400...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2245...  0.1540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2184...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2489...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2398...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.3070...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2489...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2442...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2715...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2382...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2529...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2581...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2600...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2787...  0.1660 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2365...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.2987...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2630...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2812...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2623...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2607...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2894...  0.1540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2572...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2422...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.2983...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2697...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.2983...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2765...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2718...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2613...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2765...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2792...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2536...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2642...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2532...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.2920...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2895...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2995...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2423...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2701...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2800...  0.1640 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2600...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2442...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2171...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2679...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2287...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2657...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2389...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2552...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2357...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2556...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2375...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2408...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2262...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2685...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2436...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2447...  0.1720 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2349...  0.1660 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2325...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2395...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2712...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2677...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2196...  0.1660 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2317...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2412...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2621...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2487...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2551...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2534...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2521...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2480...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2649...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2682...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2549...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2741...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2411...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2623...  0.1630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2624...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2515...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2319...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2297...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2720...  0.1531 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2690...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2638...  0.1540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2549...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2585...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2306...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2172...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2631...  0.1523 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2533...  0.1517 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2272...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2705...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2736...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2486...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2187...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2056...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2458...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2732...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2722...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2633...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2619...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2786...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2794...  0.1511 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2610...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2693...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3133...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2749...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2610...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2929...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2463...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2745...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2617...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.2899...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2894...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2479...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2286...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2363...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2716...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2469...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2509...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2547...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2609...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2531...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2241...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2808...  0.1680 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2895...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2609...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2547...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2625...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2523...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2602...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2834...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3221...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2734...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2604...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2419...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2470...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2940...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2566...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2597...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2315...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2604...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2956...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2246...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2409...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2518...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2678...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2514...  0.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2555...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2592...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2413...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2756...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2616...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2458...  0.1500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2697...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2361...  0.1540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2319...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2562...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2337...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2185...  0.1550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2558...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2514...  0.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2422...  0.1540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3943...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2822...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2535...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2856...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2393...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2270...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2494...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2441...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2642...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2519...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2402...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2446...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2619...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2615...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2439...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2329...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2654...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2816...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2575...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2757...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2532...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2753...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2450...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2803...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2634...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2103...  0.1670 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2317...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2713...  0.1540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2696...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2704...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2368...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2286...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2634...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2622...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2324...  0.1714 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2609...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2406...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2107...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2099...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2431...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2309...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.2939...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2457...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2248...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2606...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2361...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2416...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2412...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2445...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2724...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2188...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2893...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2546...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2564...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2546...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2537...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2650...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2432...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2361...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2880...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2632...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.2939...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2732...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2592...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2590...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2641...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2670...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2392...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2541...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2467...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.2899...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2747...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2781...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2284...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2534...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2754...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2491...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2366...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2050...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2520...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2152...  0.1521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2495...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2151...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2464...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2307...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2484...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2220...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2339...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2166...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2499...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2327...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2358...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2241...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2129...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2304...  0.1620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2579...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2496...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2185...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2308...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2242...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2490...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2319...  0.1480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2376...  0.1540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2393...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2386...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2457...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2501...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2523...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2310...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2566...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2289...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2532...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2438...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2314...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2232...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2174...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2523...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2573...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2419...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2437...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2390...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2130...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.1949...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2397...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2269...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.2073...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2539...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2500...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2252...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.2026...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.2010...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2292...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2766...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2535...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2558...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2481...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2703...  0.1501 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2700...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2579...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2532...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2945...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2568...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2494...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2722...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2328...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2651...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2529...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2908...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2911...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2421...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2177...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2263...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2639...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2420...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2427...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2375...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2544...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2475...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2168...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2716...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2691...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2535...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2465...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2509...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2402...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2412...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2650...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3052...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2631...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2410...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2400...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2329...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2792...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2535...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2632...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2287...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2356...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2906...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2152...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2244...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2335...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2474...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2427...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2322...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2427...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2307...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2688...  0.1520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2511...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2486...  0.1530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2523...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2183...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2337...  0.1550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2515...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2258...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2097...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2512...  0.1500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2360...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2364...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3815...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2734...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2406...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2649...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2227...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.2045...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2360...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2455...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2480...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2373...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2336...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2368...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2480...  0.1620 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2623...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2383...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2243...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2621...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2583...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2419...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2631...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2386...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2573...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2395...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2625...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2410...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2057...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2154...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2607...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2499...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2589...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2228...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2141...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2510...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2474...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2310...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2425...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2173...  0.1708 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.1978...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.1930...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2207...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2225...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2908...  0.1511 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2214...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2090...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2466...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2200...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2202...  0.1780 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2370...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2309...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2604...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2202...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2717...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2526...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2440...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2238...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2385...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2525...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2291...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2207...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2748...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2482...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2814...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2647...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2540...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2409...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2568...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2580...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2222...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2457...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2233...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2779...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2614...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2698...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2286...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2472...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2556...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2300...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2319...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.1974...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2528...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.2051...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2414...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2153...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2420...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2203...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2372...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2205...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2170...  0.1640 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2132...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2481...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2092...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2240...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2046...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2169...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2239...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2404...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2380...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.2077...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2211...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2181...  0.1670 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2247...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2229...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2428...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2224...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2402...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2300...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2475...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2531...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2269...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2543...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2137...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2424...  0.1630 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2423...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2121...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2136...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.2084...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2425...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2460...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2274...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2374...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2273...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.1973...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1935...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2315...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2191...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1994...  0.1640 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2325...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2353...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2219...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1998...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1819...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2211...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2534...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2475...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2463...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2372...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2540...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2589...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2465...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2350...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2824...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2418...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2327...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2616...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2223...  0.1650 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2512...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2481...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2725...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2651...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2291...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2091...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2063...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2415...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2252...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2262...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2283...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2343...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2215...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.2081...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2500...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2604...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2422...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2230...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2393...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2213...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2370...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2582...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2900...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2499...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2382...  0.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2250...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2227...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2623...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2451...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2464...  0.1550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2142...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2298...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2747...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2129...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.2066...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2226...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2435...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2266...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2189...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2209...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2066...  0.1501 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2580...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2361...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2321...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2359...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2041...  0.1530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2130...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2440...  0.1520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2160...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.2064...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2368...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2321...  0.1510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2261...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3617...  0.1700 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2456...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2233...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2570...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2169...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.1962...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2298...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2270...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2334...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2307...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2205...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2331...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2429...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2401...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2125...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2045...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2504...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2582...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2233...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2548...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2206...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2570...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2264...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2479...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2363...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1900...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.1980...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2543...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2432...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2512...  0.1549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2208...  0.1505 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2015...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2371...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2430...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2178...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2431...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2069...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1913...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1859...  0.1760 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2103...  0.1545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.2072...  0.1609 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2716...  0.1571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2173...  0.1541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.2006...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2413...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2118...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2101...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2096...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2250...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2490...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.1999...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2708...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2307...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2334...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2224...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2267...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2333...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2164...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2090...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2610...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2295...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2675...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2513...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2311...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2288...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2342...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2442...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2264...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2331...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2174...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2760...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2571...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2586...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2145...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2379...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2479...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2249...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2243...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1858...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2328...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1901...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2247...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.1990...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2251...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.2042...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2259...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2058...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2064...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.1949...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2296...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2102...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2214...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.1982...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.1972...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2075...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2364...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2276...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.1886...  0.1540 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2089...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.2081...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2213...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2195...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2271...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2130...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2164...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2202...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2346...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2293...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2131...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2390...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2121...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2296...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2278...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2187...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2030...  0.1620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1912...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2260...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2359...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2129...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2201...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2272...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.1882...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1877...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2212...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2193...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1833...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2219...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2395...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2158...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1907...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1830...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2064...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2453...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2284...  0.1630 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2295...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2275...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2495...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2512...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2388...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2368...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2724...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2505...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2247...  0.1700 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2579...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2120...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2435...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2346...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2664...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2515...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2226...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.2017...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.2016...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2298...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2209...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2146...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2235...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2204...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2215...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.2030...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2604...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2435...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2247...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2125...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2297...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2258...  0.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2264...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2444...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2840...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2431...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2220...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2245...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2119...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2576...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2320...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2376...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.2007...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2179...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2716...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.2033...  0.1620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.1948...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2112...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2271...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2257...  0.1538 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2124...  0.1525 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2166...  0.1592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.2086...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2477...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2238...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2246...  0.1592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2242...  0.1492 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.2021...  0.1530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.2015...  0.1540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2297...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2060...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1821...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2252...  0.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2148...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2188...  0.1560 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3960_l512.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i3960_l512.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3960_l512.ckpt\n",
      "Farther that in the\n",
      "days of her subjects of the first teash, and she had seen their soul,\n",
      "because he was in his wife, and he had been and she seemed to see her that the\n",
      "man had that lade to be simply angry. At the same as a maturing of husband\n",
      "to bring the best tervors. He had the prayer was so ashes of simply than\n",
      "he would be delighted; and that he would no more her soft and all, and\n",
      "went to his brother, and was several to the proform of his wheels and\n",
      "attention of anyence.\n",
      "\n",
      "\"If you can did not certain you and so so sat down,\" she asked.\n",
      "\n",
      "\"I have been all right, to be at the poor of all the chorse. Thyse confries\n",
      "is not at abternating in that subject, that I can't both her one arouse,\n",
      "have the party of the figure of the such thought.\n",
      "I think it's a sensable condition of that is sometices another\n",
      "to home, that he has suppressed. That's the sace well all were to be in sicture\n",
      "in which it he have to be controlled to her, but it was in the carriage,\n",
      "and he can do, and talk of this attention would not be at all.\"\n",
      "\n",
      "\"Not as a first time on the man while he was not and as such an\n",
      "emptainment of this middle when I did not shall get a such of mather.\n",
      "There's something.\"\n",
      "\n",
      "And they he was thinking of that he can't say it and to say about the\n",
      "steps of serious tend of many table. She had not tried to drop to her. He sat\n",
      "of in his, that that he could, and he saw impossible to arrively the creater\n",
      "handed over the ball when he had anrone in the secretary which was along\n",
      "to a prict of heart and the post, and her fatered same whenese he\n",
      "had sent him a song, and that he had a complication that was such always\n",
      "and house husband. He stopped her face, were several times. They changed\n",
      "it it.\n",
      "\n",
      "\"Yes, the same and so married they were thinking of all this for him would\n",
      "break of a story. That's a second,\" said Levin, and the subject of her\n",
      "sunshine was as affair, that this was that their hours all he had been\n",
      "telling her in herself to the part of the sound of this.\n",
      "\n",
      "\"What has he said, that's what\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
